import cv2
import numpy as np
import torch

# Load YOLOv5 model (yolov5m in this case)
model = torch.hub.load('ultralytics/yolov5', 'custom', path='D:\\BYTE\\Machine Learning\\Task2\\yolov5m.pt', force_reload=True)

# Function to process the video
def process_video(video_path, polygon_points, conf_threshold=0.25):
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print(f"Error: Could not open video {video_path}")
        return

    # Get video properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frame_size = (width, height)

    # Setup video writer for output
    output_path = 'D:\\BYTE\\Machine Learning\\Task2\\output_vehicle_detection.mp4'
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, frame_size)

    if not out.isOpened():
        print("Error: Could not open VideoWriter.")
        return

    # Initialize vehicle counts
    vehicle_count = {"car": 0, "bus": 0, "truck": 0}

    def draw_polygon(frame, polygon_points):
        return cv2.polylines(frame, [polygon_points], isClosed=True, color=(0, 255, 255), thickness=2)

    def is_in_polygon(point, polygon_points):
        return cv2.pointPolygonTest(polygon_points, point, False) >= 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Get YOLOv5 predictions
        results = model(frame)

        # Extract detections
        for detection in results.xyxy[0]:
            x1, y1, x2, y2, conf, cls = detection

            if conf < conf_threshold:
                continue

            # Calculate the center of the detected object
            center_x, center_y = int((x1 + x2) / 2), int((y1 + y2) / 2)

            # Check if the vehicle is within any of the polygon regions
            for polygon in polygon_points:
                if is_in_polygon((center_x, center_y), polygon):
                    label = model.names[int(cls)]

                    if label in vehicle_count:
                        vehicle_count[label] += 1

                    # Draw bounding box and label
                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                    cv2.putText(frame, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)

        # Draw the polygon regions
        for polygon in polygon_points:
            draw_polygon(frame, polygon)

        # Display the vehicle count
        count_text = f"Cars: {vehicle_count['car']} | Buses: {vehicle_count['bus']} | Trucks: {vehicle_count['truck']}"
        cv2.putText(frame, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)

        # Write the processed frame to the output video
        out.write(frame)

        # Display the frame (optional)
        cv2.imshow('Vehicle Detection', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

    print("Video processing complete. Output saved at:", output_path)

# Define the video path and polygon coordinates
video_path = 'D:\\BYTE\\Machine Learning\\Task2\\Task2.mp4'
polygon_points = [
    np.array([(656, 2019), (1606, 2016), (1612, 1442), (681, 1462)
              ], np.int32),
    np.array([(2015, 1999), (3167, 1973), (2761, 1055), (1990, 1053)], np.int32),
    np.array([(3156, 1690), (3773, 1523), (3450, 1257), (3100, 1327)], np.int32)
]

# Call the function to process the video
process_video(video_path, polygon_points, conf_threshold=0.25)


#Navdeep this side for any problem contact 7982447979